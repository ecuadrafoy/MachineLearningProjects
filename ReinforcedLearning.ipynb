{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtClaQlx7R6OFI2ZWvxtFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecuadrafoy/PracticalMachineLearning/blob/master/ReinforcedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBiEQyiHdY8V",
        "colab_type": "text"
      },
      "source": [
        "# Using `tf_agents` to train a Deep Q Network\n",
        "\n",
        "* `tf_agents` is a library for RL based on Tensorflow. \n",
        "* TF- agents runs on TensorFlow 2.2.0 and \n",
        "* The Deep Q Newtork algorith, developed in 2015 enhanced a classic Q-Learning algorithm with deep neural netowkrs and a technique called experience replay.\n",
        "* The Q-function of a policy $\\pi$ measures the expected return or discounted sum of rewards obtained from the state $s$ by taking action $a$ frist and following policy $\\pi$ thereafter.  Optimal Q-function is defined as $Q^*(s,a)$ as the maximum return that can be obtained starting from observation *s*, taking action *a* and following the optimal policy thereafter.\n",
        "* *S* is the set of states. At each time step *t*, the agents gets the environment's state - *St*, where *St* $\\epsilon$ *S*\n",
        "* *A* is a set of actions that the agent can do in defined state. The agent makes the decision to perform an action, based on the state *St* - *At* where *At* $\\epsilon$ *A(St)*. *A(St)* represents a set of possible actions in the state *St*\n",
        "\n",
        "* The optimal Q-function obeys the following optimality equation:\n",
        "\n",
        "$\\begin{equation}Q^\\ast(s, a) = \\mathbb{E}[ r + \\gamma \\max_{a'} Q^\\ast(s', a') ]\\end{equation}$\n",
        "\n",
        "This means that the maximum return from state $s$ and action $a$ is the sum of the immediate reward $r$ and the return (discounted by $\\gamma$) obtained by following the optimal policy therafter until the end of the episode\n",
        "\n",
        "* The discount factour $\\gamma$ determines how much importance we want to give to future rewards.\n",
        "\n",
        "* To avoid computing the full expectation in the DQN loss, it can be minimized using stochastic gradient descent. If the loss is computed using just the last transition $\\{s, a, r, s'\\}$, this reduces to standard Q-Learning. \n",
        "\n",
        "* At each time step of data collection, the transitions are added to a circular buffer called the *replay buffer*. Then during training, instead of using just the latest transition to compute the loss and its gradient, it is computed using a mini-batch of transitions sampled from the replay buffer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWauZWWDw8NN",
        "colab_type": "text"
      },
      "source": [
        "## Setting up Packages\n",
        "\n",
        "* This experiments implements a DQN agent as developed by Mnhi et al. 2015. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpImxpvDCvn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cb52497-2246-4c9d-9285-8af0ad74e34a"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 784 kB in 0s (7,165 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144465 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting gym==0.10.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.11) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-cp36-none-any.whl size=1588313 sha256=52038bcd7aa6dd323b3f2356085f8b6e15d4c620b999a897611b2b129a47c430\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.17.2\n",
            "    Uninstalling gym-0.17.2:\n",
            "      Successfully uninstalled gym-0.17.2\n",
            "Successfully installed gym-0.10.11\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (7.0.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp36-none-any.whl size=3303880 sha256=784c73c4becedb9d8ce333a70bc8161a665690de50a91c2260e67fe27938117d\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Installing collected packages: pyglet\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "Successfully installed pyglet-1.3.2\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b0/88c9aab39050cfb544ec73ee48b8d0e67b4b16ed5470c82235255b119952/tf_agents-0.5.0-py3-none-any.whl (933kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.2)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n",
            "Collecting gin-config==0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/be/c984b1c8a7ba1c385b32bf39c7a225cd9f713d49705898309d01b60fd0e7/gin_config-0.1.3-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-probability>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (49.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents) (1.3.0)\n",
            "Installing collected packages: gin-config, tf-agents\n",
            "  Found existing installation: gin-config 0.3.0\n",
            "    Uninstalling gin-config-0.3.0:\n",
            "      Successfully uninstalled gin-config-0.3.0\n",
            "Successfully installed gin-config-0.1.3 tf-agents-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIztt5A7w7ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyXkpy9tyHwo",
        "colab_type": "text"
      },
      "source": [
        "## Create the environment\n",
        "* An environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites\n",
        "* In CartPole we have a cart with a pole on top of it. The mission is to learn to keep up the pole moving the cart left and right. \n",
        "* The enviroment from suite_gym includes a slightly customized version that is optimized for its use with TF-Agents\n",
        "* `TFPyEnvironment()` converts numpy arrays for state observations, actions and rewards into TensorFlow tensors. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVsDdHCIK7nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = suite_gym.load('CartPole-v1')\n",
        "\n",
        "env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK1sS5gPqNSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kaeq7dlsk-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ab0903e8-4b2e-484b-814a-8cfd92e1f82d"
      },
      "source": [
        "env.reset()\n",
        "#PIL.Image.fromarray(env.render())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
              "array([[ 0.02739406, -0.03622656,  0.03145718,  0.04715188]],\n",
              "      dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvAgS8NcqdfS",
        "colab_type": "text"
      },
      "source": [
        "* The `environment.step` method takes an action in the environment and returns a `TimeStep` tuple containing the next observation of the enviroment and the reward for the action\n",
        "\n",
        "* The `time_step_spec()` method returns the specification of the `TimeStep` tuple. Its observation attribute shows the shape of observations, the data types, and the ranges of allowed values. reward attribute shows the same details for the reward\n",
        "\n",
        "* The `action_spec()` method returns the shape, data types and allowed values of valid actions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQkvoxWs7RG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b5db62d1-d448-4a23-e5d5-b3684dfc3487"
      },
      "source": [
        "print('Observation Spec:')\n",
        "env.time_step_spec().observation"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Spec:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
              "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
              "      dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o18gwtbTs_fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a77c3a6-855b-4346-9a9a-f9c88203b1db"
      },
      "source": [
        "print('Reward Spec:')\n",
        "env.time_step_spec().reward"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward Spec:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorSpec(shape=(), dtype=tf.float32, name='reward')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcs4MCPStCLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d8dc2e6-b4a5-4568-fe59-b69b8b731eda"
      },
      "source": [
        "print('Action Spec:')\n",
        "env.action_spec()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Spec:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTXwjjk_h55Y",
        "colab_type": "text"
      },
      "source": [
        "## Agent\n",
        "\n",
        "* There are different agents in TF-Agents. Here DQN will be used.\n",
        "* One of the main parameters of the agents is its Q neural network which will be used to calculate Q-values for the actions in each step.\n",
        "* A Q network has to parameters, input_tensor_spec and action_spec which define observation shape and the action shape\n",
        "* The agent also requires an optimizer to find the values for the Q network parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk9pRQVZr7CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_net = q_network.QNetwork(env.observation_spec(), \n",
        "                           env.action_spec())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uALyZt8RibIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMn-5kmtimv3",
        "colab_type": "text"
      },
      "source": [
        "The agent is defined and initialized with the following\n",
        "\n",
        "* `time_step_spec` that is obtained from the environment and defines how time steps are defind\n",
        "* `action_spec`\n",
        "* The Q network created prior\n",
        "* The optimizer\n",
        "* Train step counter that is a rank 0 tensor which will count the number of steps done on the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8vid701jFU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step_counter = tf.Variable(0)\n",
        "agent = dqn_agent.DqnAgent(env.time_step_spec(),\n",
        "                           env.action_spec(),\n",
        "                           q_network=q_net,\n",
        "                           optimizer=optimizer,\n",
        "                           td_errors_loss_fn= \n",
        "                                  common.element_wise_squared_loss,\n",
        "                           train_step_counter=train_step_counter)\n",
        "agent.initialize()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfYcrjcVjdKx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "* The following function computes the average return of a policy, given the policy, environment, and a number of episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkNXYjRAjglh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(env, agent.policy, 5)\n",
        "returns = [avg_return]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3GcZGevjnIf",
        "colab_type": "text"
      },
      "source": [
        "* As described previously, one of the breakthroughs of DQN was experience replay, in which the experiences of the agent (state, action, reward) are stored and used to train the Q network in batches in each step\n",
        "* TF-Agents include the object `TFUniformReplayBuffer` which stores these experiences to re-use them later.\n",
        "* In this method, an environment, a policy and a buffer are taken, the current time_step is formed by its state observation and reward at that time_step, the action the policy chooses and then the next time_step. This is then stored in the replay buffer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQKoXz3kKns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                                data_spec=agent.collect_data_spec,                                                                \n",
        "                                batch_size=env.batch_size,                                                              \n",
        "                                max_length=100000)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnN8F531uZke",
        "colab_type": "text"
      },
      "source": [
        "## Policies\n",
        "\n",
        "* A policy defines the way an agent acts in an environment. The goal of RL is to train the model until the policy produces the desired outcome.\n",
        "  * The goal of CartPole is to keep the pole balanced upright over the cart\n",
        "  * The policy returns an action for each `time_step` observation\n",
        "* Agents contain two policies\n",
        "  * `agent.policy` the main policy used for evaluation and deployment\n",
        "  * `agent.collect_policy` a second policy used for data collection\n",
        "\n",
        "* Policies can be created independent of agents using `random_tf_policy` to create a policy which will randomly select an action for each `time_step`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ncbfq98u6Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
        "                                                env.action_spec())\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5ckuiqJksAf",
        "colab_type": "text"
      },
      "source": [
        "* The collect_step executes the random policy in the environment for a few steps and records the data in the replay buffer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XDd3s19krbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zh92M3tkLap",
        "colab_type": "text"
      },
      "source": [
        "## Training the Agent\n",
        "\n",
        "* The number of steps made will be defined and after this number of steps the agent will be trained in every iteration, modifying its policy. \n",
        "\n",
        "* the batch size is define with which the Q network will be trained and the iterator is used so that it can iterate over the experience of the agent. \n",
        "\n",
        "* Get experience by acting on the environment, train policy and repeat\n",
        "\n",
        "* The loss and the performance are printed every 200 and 1000 steps respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iP5mNe8l_bG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4752e025-fcb8-48b7-8ac7-91d92749ce3b"
      },
      "source": [
        "collect_steps_per_iteration = 1\n",
        "batch_size = 64\n",
        "dataset = replay_buffer.as_dataset(num_parallel_calls=3, \n",
        "                                   sample_batch_size=batch_size, \n",
        "                                   num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "num_iterations = 20000\n",
        "env.reset()\n",
        "for _ in range(batch_size):\n",
        "    collect_step(env, agent.policy, replay_buffer)\n",
        "for _ in range(num_iterations):\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    for _ in range(collect_steps_per_iteration):\n",
        "        collect_step(env, agent.collect_policy, replay_buffer)\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "    step = agent.train_step_counter.numpy()\n",
        "    # Print loss every 200 steps.\n",
        "    if step % 200 == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "    # Evaluate agent's performance every 1000 steps.\n",
        "    if step % 1000 == 0:\n",
        "        avg_return = compute_avg_return(env, agent.policy, 5)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "        returns.append(avg_return)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 200: loss = 12.630553245544434\n",
            "step = 400: loss = 2.316990375518799\n",
            "step = 600: loss = 0.38659077882766724\n",
            "step = 800: loss = 3.7462635040283203\n",
            "step = 1000: loss = 1.9752943515777588\n",
            "step = 1000: Average Return = 84.5999984741211\n",
            "step = 1200: loss = 115.80011749267578\n",
            "step = 1400: loss = 54.5186653137207\n",
            "step = 1600: loss = 66.94447326660156\n",
            "step = 1800: loss = 37.08972930908203\n",
            "step = 2000: loss = 347.9102783203125\n",
            "step = 2000: Average Return = 53.599998474121094\n",
            "step = 2200: loss = 162.85171508789062\n",
            "step = 2400: loss = 591.3302001953125\n",
            "step = 2600: loss = 1506.765869140625\n",
            "step = 2800: loss = 542.5953369140625\n",
            "step = 3000: loss = 4155.16015625\n",
            "step = 3000: Average Return = 182.39999389648438\n",
            "step = 3200: loss = 6408.60595703125\n",
            "step = 3400: loss = 2877.0576171875\n",
            "step = 3600: loss = 2520.786865234375\n",
            "step = 3800: loss = 3660.02294921875\n",
            "step = 4000: loss = 613.3059692382812\n",
            "step = 4000: Average Return = 152.8000030517578\n",
            "step = 4200: loss = 3717.43603515625\n",
            "step = 4400: loss = 746.3798828125\n",
            "step = 4600: loss = 5438.10546875\n",
            "step = 4800: loss = 765.2066650390625\n",
            "step = 5000: loss = 459.37957763671875\n",
            "step = 5000: Average Return = 179.0\n",
            "step = 5200: loss = 5544.63671875\n",
            "step = 5400: loss = 974.0106201171875\n",
            "step = 5600: loss = 859.83251953125\n",
            "step = 5800: loss = 2448.12939453125\n",
            "step = 6000: loss = 909.6633911132812\n",
            "step = 6000: Average Return = 210.60000610351562\n",
            "step = 6200: loss = 439.3554382324219\n",
            "step = 6400: loss = 524.2734375\n",
            "step = 6600: loss = 358.22021484375\n",
            "step = 6800: loss = 435.38128662109375\n",
            "step = 7000: loss = 344.36883544921875\n",
            "step = 7000: Average Return = 372.6000061035156\n",
            "step = 7200: loss = 3235.189697265625\n",
            "step = 7400: loss = 3239.28759765625\n",
            "step = 7600: loss = 365.0747375488281\n",
            "step = 7800: loss = 233.364990234375\n",
            "step = 8000: loss = 2245.968017578125\n",
            "step = 8000: Average Return = 242.0\n",
            "step = 8200: loss = 318.326904296875\n",
            "step = 8400: loss = 157.3310546875\n",
            "step = 8600: loss = 154.67578125\n",
            "step = 8800: loss = 397.8871765136719\n",
            "step = 9000: loss = 333.2767333984375\n",
            "step = 9000: Average Return = 389.20001220703125\n",
            "step = 9200: loss = 930.0458984375\n",
            "step = 9400: loss = 692.085205078125\n",
            "step = 9600: loss = 3595.54248046875\n",
            "step = 9800: loss = 1764.85400390625\n",
            "step = 10000: loss = 1095.087890625\n",
            "step = 10000: Average Return = 500.0\n",
            "step = 10200: loss = 9701.2568359375\n",
            "step = 10400: loss = 324751.5625\n",
            "step = 10600: loss = 17456.43359375\n",
            "step = 10800: loss = 151304.859375\n",
            "step = 11000: loss = 13978.998046875\n",
            "step = 11000: Average Return = 500.0\n",
            "step = 11200: loss = 275315.625\n",
            "step = 11400: loss = 51096.09375\n",
            "step = 11600: loss = 27944.22265625\n",
            "step = 11800: loss = 30772.748046875\n",
            "step = 12000: loss = 54336.83984375\n",
            "step = 12000: Average Return = 500.0\n",
            "step = 12200: loss = 60453.9375\n",
            "step = 12400: loss = 92053.5546875\n",
            "step = 12600: loss = 545703.875\n",
            "step = 12800: loss = 1638907.5\n",
            "step = 13000: loss = 61279.9609375\n",
            "step = 13000: Average Return = 500.0\n",
            "step = 13200: loss = 84603.3125\n",
            "step = 13400: loss = 2635401.25\n",
            "step = 13600: loss = 123076.953125\n",
            "step = 13800: loss = 149203.984375\n",
            "step = 14000: loss = 129841.609375\n",
            "step = 14000: Average Return = 500.0\n",
            "step = 14200: loss = 164424.796875\n",
            "step = 14400: loss = 318700.71875\n",
            "step = 14600: loss = 142125.875\n",
            "step = 14800: loss = 2528416.75\n",
            "step = 15000: loss = 238624.40625\n",
            "step = 15000: Average Return = 500.0\n",
            "step = 15200: loss = 189963.671875\n",
            "step = 15400: loss = 376696.375\n",
            "step = 15600: loss = 279306.1875\n",
            "step = 15800: loss = 372488.375\n",
            "step = 16000: loss = 9701646.0\n",
            "step = 16000: Average Return = 500.0\n",
            "step = 16200: loss = 320479.21875\n",
            "step = 16400: loss = 2488128.25\n",
            "step = 16600: loss = 281343.09375\n",
            "step = 16800: loss = 634004.125\n",
            "step = 17000: loss = 1361626.5\n",
            "step = 17000: Average Return = 470.0\n",
            "step = 17200: loss = 12583675.0\n",
            "step = 17400: loss = 2565791.0\n",
            "step = 17600: loss = 14476139.0\n",
            "step = 17800: loss = 14492344.0\n",
            "step = 18000: loss = 737794.625\n",
            "step = 18000: Average Return = 500.0\n",
            "step = 18200: loss = 560066.625\n",
            "step = 18400: loss = 941686.875\n",
            "step = 18600: loss = 582790.5\n",
            "step = 18800: loss = 984821.75\n",
            "step = 19000: loss = 773086.5625\n",
            "step = 19000: Average Return = 500.0\n",
            "step = 19200: loss = 23805720.0\n",
            "step = 19400: loss = 1338159.75\n",
            "step = 19600: loss = 1134957.5\n",
            "step = 19800: loss = 1008536.375\n",
            "step = 20000: loss = 1065453.125\n",
            "step = 20000: Average Return = 468.3999938964844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPULWh8BmKV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8cc4038f-b1f3-48c8-832b-7a6d0ca9c26a"
      },
      "source": [
        "iterations = range(0, num_iterations + 1, 1000)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}